# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YUz6lUPzrX6i4fVZL48pSJJYb79HDv_n
"""

# This is a complete Google Colab notebook for Traffic Density Estimation using CNN.
# Copy this entire code into a new Colab notebook and run cell by cell.
# This version creates a dummy dataset with synthetic images, so no Kaggle dataset or kaggle.json is required.
# The notebook includes dataset creation, preprocessing, model building, training, evaluation, and prediction demo.

# Cell 1: Markdown - Title and Introduction
"""
# Traffic Density Estimation with CNN (Dummy Dataset)

This notebook implements a complete deep learning project for estimating traffic density using a Convolutional Neural Network (CNN).
Since no external dataset is used, it generates a dummy dataset of synthetic images with random rectangles representing vehicles.
Density is classified as low (0-3 vehicles), medium (4-8 vehicles), or high (9+ vehicles).

The project includes dataset creation, preprocessing, model building, training, evaluation, and prediction demo.
"""

# Cell 2: Install required packages
!pip install tensorflow opencv-python scikit-learn matplotlib

# Cell 3: Import libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import LabelBinarizer
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.utils import to_categorical
from google.colab import files

# Cell 4: Create Dummy Dataset
# Function to create a synthetic image with random rectangles as "vehicles"
def create_dummy_image(num_vehicles, img_size=224):
    img = np.zeros((img_size, img_size, 3), dtype=np.uint8)
    for _ in range(num_vehicles):
        x, y = np.random.randint(10, img_size-10, 2)
        # Draw white rectangles to represent vehicles
        cv2.rectangle(img, (x-10, y-5), (x+10, y+5), (255, 255, 255), -1)
    return img

# Generate dummy dataset
num_samples = 600  # Total images
image_paths = []
vehicle_counts = []

for i in range(num_samples):
    count = np.random.randint(0, 15)  # Random number of vehicles (0 to 14)
    img = create_dummy_image(count)
    path = f'dummy_{i}.jpg'
    cv2.imwrite(path, img)
    image_paths.append(path)
    vehicle_counts.append(count)

# Create DataFrame
df = pd.DataFrame({'image_path': image_paths, 'count': vehicle_counts})

# Define density classes: low (0-3), medium (4-8), high (9+)
bins = [0, 4, 9, np.inf]
labels = ['low', 'medium', 'high']
df['density'] = pd.cut(df['count'], bins=bins, labels=labels, right=False)

# Display class distribution
print("Dummy dataset created.")
print("Vehicle count statistics:")
print(df['count'].describe())
print("\nClass distribution:")
print(df['density'].value_counts())

# Cell 5: Preprocessing - Load images, resize, normalize, split, augmentation
# Parameters
img_size = 224
batch_size = 32

# Load and preprocess images
def load_image(path):
    img = cv2.imread(path)
    img = cv2.resize(img, (img_size, img_size))
    img = img.astype('float32') / 255.0  # Normalize to [0,1]
    return img

# Load all images
X = np.array([load_image(p) for p in df['image_path']])
lb = LabelBinarizer()
y = lb.fit_transform(df['density'])
num_classes = len(lb.classes_)

# Train-test split (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=df['density'], random_state=42
)

# Data augmentation for training
train_datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)

# Validation generator without augmentation
val_datagen = ImageDataGenerator()
val_generator = val_datagen.flow(X_test, y_test, batch_size=batch_size)

# Cell 6: Model Building - CNN Architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 3)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.summary()

# Cell 7: Training - Compile and train the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Callbacks
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)

# Train
epochs = 50
steps_per_epoch = len(X_train) // batch_size
validation_steps = len(X_test) // batch_size

history = model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    validation_data=val_generator,
    validation_steps=validation_steps,
    callbacks=[checkpoint, early_stop]
)

# Load best model
model.load_weights('best_model.h5')

# Cell 8: Evaluation - Accuracy, confusion matrix, classification report
# Evaluate on test set
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_acc:.2f}')

# Predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Confusion matrix
cm = confusion_matrix(y_true, y_pred_classes)
print('Confusion Matrix:')
print(cm)

# Classification report
print('Classification Report:')
print(classification_report(y_true, y_pred_classes, target_names=lb.classes_))

# Cell 9: Plot training/validation curves
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(acc))

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend()
plt.title('Training and Validation Loss')
plt.show()

# Cell 10: Prediction Demo - Test on sample images
# Select 5 random test images
indices = np.random.choice(len(X_test), 5)
samples = X_test[indices]
true_labels = lb.classes_[y_true[indices]]
pred_probs = model.predict(samples)
pred_labels = lb.classes_[np.argmax(pred_probs, axis=1)]

# Plot
plt.figure(figsize=(15, 5))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(samples[i])
    plt.title(f'True: {true_labels[i]}\nPred: {pred_labels[i]}')
    plt.axis('off')
plt.show()

# Cell 11: Cleanup - Remove dummy images
for p in image_paths:
    if os.path.exists(p):
        os.remove(p)
print("Dummy images cleaned up.")